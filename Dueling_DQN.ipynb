{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSPpKLU97MSm"
      },
      "source": [
        "# Installing the important libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT7fTAWKXHl3"
      },
      "outputs": [],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otqrTw6l7Uis"
      },
      "source": [
        "# Importing the important modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G41GVG9LcXiv"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "plt.ion()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhOwyKe47a3d"
      },
      "source": [
        "# Implementation of the Car Racing game from Gymnasium using Deep Q Learning with CNN\n",
        "\n",
        "Next steps:\n",
        "*   Customize the Car Environment\n",
        "*   Define the CNN architecture\n",
        "*   Define the Replay Memory\n",
        "*   Define the agent\n",
        "*   Train the agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRCIHWCMc_QQ"
      },
      "outputs": [],
      "source": [
        "def image_preprocessing(img):\n",
        "  img = cv2.resize(img, dsize=(84, 84))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h4ArKIrdu2c"
      },
      "outputs": [],
      "source": [
        "class CarEnvironment(gym.Wrapper):\n",
        "  def __init__(self, env, skip_frames=3, stack_frames=4, no_operation=50, **kwargs):\n",
        "    super().__init__(env, **kwargs)\n",
        "    self._no_operation = no_operation\n",
        "    self._skip_frames = skip_frames\n",
        "    self._stack_frames = stack_frames\n",
        "\n",
        "  def reset(self):\n",
        "    observation, info = self.env.reset()\n",
        "\n",
        "    for i in range(self._no_operation):\n",
        "      observation, reward, terminated, truncated, info = self.env.step(0)\n",
        "\n",
        "    observation = image_preprocessing(observation)\n",
        "    self.stack_state = np.tile(observation, (self._stack_frames, 1, 1))\n",
        "    return self.stack_state, info\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    total_reward = 0\n",
        "    for i in range(self._skip_frames):\n",
        "      observation, reward, terminated, truncated, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "    observation = image_preprocessing(observation)\n",
        "    self.stack_state = np.concatenate((self.stack_state[1:], observation[np.newaxis]), axis=0)\n",
        "    return self.stack_state, total_reward, terminated, truncated, info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk4BxqWQhKjQ"
      },
      "outputs": [],
      "source": [
        "class DuelingCNN(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.n_features = 32 * 9 * 9\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "      nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "    self.v = nn.Sequential(\n",
        "      nn.Linear(self.n_features, 256),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(256, 1)\n",
        "    )\n",
        "\n",
        "    self.a = nn.Sequential(\n",
        "        nn.Linear(self.n_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, self.out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view((-1, self.n_features))\n",
        "    val = self.v(x)\n",
        "    adv = self.a(x)\n",
        "    q = val + (adv - adv.mean())\n",
        "\n",
        "    return q\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TqOGwEXkNYA"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYzoY0D21Dno"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "  def __init__(self, action_space, batch_size=256, gamma=0.99, eps_start=0.9, eps_end=0.05, eps_decay=1000, lr=0.001):\n",
        "    self._n_observation = 4\n",
        "    self._n_actions = 5\n",
        "    self._action_space = action_space\n",
        "    self._batch_size = batch_size\n",
        "    self._gamma = gamma\n",
        "    self._eps_start = eps_start\n",
        "    self._eps_end = eps_end\n",
        "    self._eps_decay = eps_decay\n",
        "    self._lr = lr\n",
        "    self._total_steps = 0\n",
        "    self._evaluate_loss = []\n",
        "    self.network = DuelingCNN(self._n_observation, self._n_actions).to(device)\n",
        "    self.target_network = DuelingCNN(self._n_observation, self._n_actions).to(device)\n",
        "    self.target_network.load_state_dict(self.network.state_dict())\n",
        "    self.optimizer = optim.AdamW(self.network.parameters(), lr=self._lr, amsgrad=True)\n",
        "    self._memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "  def select_action(self, state, evaluation_phase=False):\n",
        "    sample = random.random()\n",
        "    eps_threshold = self._eps_end + (self._eps_start - self._eps_end) * math.exp(-1. * self._total_steps / self._eps_decay)\n",
        "    self._total_steps += 1\n",
        "    if evaluation_phase:\n",
        "      with torch.no_grad():\n",
        "        return self.target_network(state).max(1).indices.view(1, 1)\n",
        "    elif sample > eps_threshold:\n",
        "      with torch.no_grad():\n",
        "        return self.network(state).max(1).indices.view(1, 1)\n",
        "    else:\n",
        "      return torch.tensor([[self._action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    if len(self._memory) < self._batch_size:\n",
        "        return\n",
        "    transitions = self._memory.sample(self._batch_size)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    state_action_values = self.network(state_batch).gather(1, action_batch)\n",
        "\n",
        "    next_state_values = torch.zeros(self._batch_size, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1).values\n",
        "    expected_state_action_values = (next_state_values * self._gamma) + reward_batch\n",
        "\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_value_(self.network.parameters(), 100)\n",
        "    self.optimizer.step()\n",
        "\n",
        "    self._evaluate_loss.append(loss.item())\n",
        "\n",
        "    return\n",
        "\n",
        "  def copy_weights(self):\n",
        "    self.target_network.load_state_dict(self.network.state_dict())\n",
        "\n",
        "  def get_loss(self):\n",
        "    return self._evaluate_loss\n",
        "\n",
        "  def save_model(self, i):\n",
        "    torch.save(self.target_network.state_dict(), f'model_weights_{i}.pth')\n",
        "\n",
        "  def load_model(self, i):\n",
        "    self.target_network.load_state_dict(torch.load(f'model_weights_{i}.pth'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPV8PdRwA5h4"
      },
      "outputs": [],
      "source": [
        "rewards_per_episode = []\n",
        "episode_duration = []\n",
        "average_episode_loss = []\n",
        "\n",
        "episodes = 500\n",
        "C = 5\n",
        "\n",
        "env = gym.make('CarRacing-v2', continuous=False)\n",
        "n_actions = env.action_space\n",
        "agent = DQN(n_actions)\n",
        "\n",
        "\n",
        "for episode in range(1, episodes + 1):\n",
        "\n",
        "  if episode % 10 == 0:\n",
        "    print(f\"{episode} episodes done\")\n",
        "\n",
        "  env = gym.make('CarRacing-v2', continuous=False)\n",
        "  env = CarEnvironment(env)\n",
        "\n",
        "  state, info = env.reset()\n",
        "\n",
        "  state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "  episode_total_reward = 0\n",
        "\n",
        "  for t in count():\n",
        "    action = agent.select_action(state)\n",
        "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "    reward = torch.tensor([reward], device=device)\n",
        "    episode_total_reward += reward\n",
        "    done = terminated or truncated\n",
        "\n",
        "    if terminated:\n",
        "      next_state = None\n",
        "      print(\"Finished the lap successfully!\")\n",
        "    else:\n",
        "      next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    agent._memory.push(state, action, next_state, reward)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    agent.train()\n",
        "\n",
        "    if done:\n",
        "      if agent._memory.__len__() >= 128:\n",
        "        episode_duration.append(t + 1)\n",
        "        rewards_per_episode.append(episode_total_reward)\n",
        "        ll = agent.get_loss()\n",
        "        average_episode_loss.append(sum(ll) / len(ll))\n",
        "      break\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "      agent.save_model(episode)\n",
        "      with open('statistics.pkl', 'wb') as f:\n",
        "        pickle.dump((episode_duration, rewards_per_episode, average_episode_loss), f)\n",
        "\n",
        "\n",
        "  if episode % C == 0:\n",
        "    agent.copy_weights()\n",
        "\n",
        "agent.save_model(episodes)\n",
        "with open('statistics.pkl', 'wb') as f:\n",
        "  pickle.dump((episode_duration, rewards_per_episode, average_episode_loss), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR_KpaPU9iUd"
      },
      "source": [
        "# Evaluation of the agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5cVtbphvX7x"
      },
      "outputs": [],
      "source": [
        "def plot_statistics(x, y, title, x_axis, y_axis):\n",
        "  plt.plot(x, y)\n",
        "  plt.xlabel(x_axis)\n",
        "  plt.ylabel(y_axis)\n",
        "  plt.title(title)\n",
        "  plt.grid(True)\n",
        "  plt.savefig(f'{title}.png')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfNnnJUfemOt"
      },
      "outputs": [],
      "source": [
        "eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
        "eval_env = CarEnvironment(eval_env)\n",
        "\n",
        "frames = []\n",
        "scores = 0\n",
        "s, _ = eval_env.reset()\n",
        "\n",
        "done, ret = False, 0\n",
        "\n",
        "while not done:\n",
        "    frames.append(eval_env.render())\n",
        "    s = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    a = agent.select_action(s, evaluation_phase=True)\n",
        "    discrete_action = a.item() % 5\n",
        "    s_prime, r, terminated, truncated, info = eval_env.step(discrete_action)\n",
        "    s = s_prime\n",
        "    ret += r\n",
        "    done = terminated or truncated\n",
        "    if terminated:\n",
        "      print(terminated)\n",
        "scores += ret\n",
        "\n",
        "\n",
        "def animate(imgs, video_name, _return=True):\n",
        "    import cv2\n",
        "    import os\n",
        "    import string\n",
        "    import random\n",
        "\n",
        "    if video_name is None:\n",
        "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
        "    height, width, layers = imgs[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
        "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
        "\n",
        "    for img in imgs:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        video.write(img)\n",
        "    video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi1bJ3j_k6U3"
      },
      "outputs": [],
      "source": [
        "animate(frames, None)\n",
        "\n",
        "with open('statistics.pkl', 'rb') as f:\n",
        "    data_tuple = pickle.load(f)\n",
        "\n",
        "episode_duration, rewards_per_episode, average_episode_loss = data_tuple\n",
        "\n",
        "x = [k for k in range(episodes)]\n",
        "\n",
        "plot_statistics(x, rewards_per_episode, \"Rewards for every episode\", \"Episode\", \"Reward\")\n",
        "plot_statistics(x, average_episode_loss, \"Average loss for every episode\", \"Episode\", \"Average Loss\")\n",
        "plot_statistics(x, episode_duration, \"Duration (in steps) for every episode\", \"Episode\", \"Duration\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}